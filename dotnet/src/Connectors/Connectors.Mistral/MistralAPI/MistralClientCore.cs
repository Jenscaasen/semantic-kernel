// Copyright (c) Microsoft. All rights reserved.

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Runtime.CompilerServices;
using System.Text;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.Mistral.API;
using Microsoft.SemanticKernel.Connectors.Mistral.MistralAPI;
using Microsoft.SemanticKernel.Http;

#pragma warning disable CA2208 // Instantiate argument exceptions correctly

namespace Microsoft.SemanticKernel.Connectors.Mistral;

/// <summary>
/// Base class for AI clients that provides common functionality for interacting with Mistral services.
/// </summary>
internal sealed class MistralClientCore
{
    /// <summary>
    /// The maximum number of auto-invokes that can be in-flight at any given time as part of the current
    /// asynchronous chain of execution.
    /// </summary>
    /// <remarks>
    /// This is a fail-safe mechanism. If someone accidentally manages to set up execution settings in such a way that
    /// auto-invocation is invoked recursively, and in particular where a prompt function is able to auto-invoke itself,
    /// we could end up in an infinite loop. This const is a backstop against that happening. We should never come close
    /// to this limit, but if we do, auto-invoke will be disabled for the current flow in order to prevent runaway execution.
    /// With the current setup, the way this could possibly happen is if a prompt function is configured with built-in
    /// execution settings that opt-in to auto-invocation of everything in the kernel, in which case the invocation of that
    /// prompt function could advertize itself as a candidate for auto-invocation. We don't want to outright block that,
    /// if that's something a developer has asked to do (e.g. it might be invoked with different arguments than its parent
    /// was invoked with), but we do want to limit it. This limit is arbitrary and can be tweaked in the future and/or made
    /// configurable should need arise.
    /// </remarks>
    private const int MaxInflightAutoInvokes = 5;

    /// <summary>Tracking <see cref="AsyncLocal{Int32}"/> for <see cref="MaxInflightAutoInvokes"/>.</summary>
    private static readonly AsyncLocal<int> s_inflightAutoInvokes = new();

    internal MistralClientCore(string modelName, string apiKey, HttpClient? httpClient = null, ILogger? logger = null)
    {
        this._apiKey = apiKey;
        this._httpClient = HttpClientProvider.GetHttpClient(httpClient);
        this.DeploymentOrModelName = modelName;
        this.Logger = logger ?? NullLogger.Instance;
    }

    /// <summary>
    /// Model Id or Deployment Name
    /// </summary>
    internal string DeploymentOrModelName { get; set; } = string.Empty;

    /// <summary>
    /// Logger instance
    /// </summary>
    internal ILogger Logger { get; set; }

    private readonly string _apiKey;
    private readonly HttpClient _httpClient;

    /// <summary>
    /// Storage for AI service attributes.
    /// </summary>
    internal Dictionary<string, object?> Attributes { get; } = new();

    /// <summary>
    /// Creates completions for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="executionSettings">Execution settings for the completion API.</param>
    /// <param name="kernel">The <see cref="Kernel"/> containing services, plugins, and other state for use throughout the operation.</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>Completions generated by the remote model</returns>
    internal async Task<IReadOnlyList<TextContent>> GetTextResultsAsync(
        string text,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {
        ChatHistory history = new();
        history.AddUserMessage(text);
        var chatResult = await this.GetChatMessageContentsAsync(history, executionSettings, kernel, cancellationToken).ConfigureAwait(false);
        return chatResult.Select(choice => new TextContent(choice.Content, this.DeploymentOrModelName, choice, Encoding.UTF8, choice.Metadata)).ToList();
    }

    internal IAsyncEnumerable<StreamingTextContent> GetStreamingTextContentsAsync(
        string prompt,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
         CancellationToken cancellationToken = default)
    {
        throw new NotImplementedException("Not supported by API");
    }

    /// <summary>
    /// Generate a new chat message
    /// </summary>
    /// <param name="chat">Chat history</param>
    /// <param name="executionSettings">Execution settings for the completion API.</param>
    /// <param name="kernel">The <see cref="Kernel"/> containing services, plugins, and other state for use throughout the operation.</param>
    /// <param name="cancellationToken">Async cancellation token</param>
    /// <returns>Generated chat message in string format</returns>
    internal async Task<IReadOnlyList<ChatMessageContent>> GetChatMessageContentsAsync(
        ChatHistory chat,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {
        MistralPromptExecutionSettings textExecutionSettings = MistralPromptExecutionSettings.FromExecutionSettings(executionSettings, MistralPromptExecutionSettings.DefaultTextMaxTokens);
        bool autoInvoke = kernel is not null && textExecutionSettings.ToolCallBehavior?.MaximumAutoInvokeAttempts > 0 && s_inflightAutoInvokes.Value < MaxInflightAutoInvokes;

        if (autoInvoke)
        {
            this.AddToolCallsToPromptExecutionSettings(textExecutionSettings, kernel);
        }

        for (int iteration = 1; ; iteration++)
        {
            // Make the request.
            MistralAIChatEndpointResponse responseData = await this.CallMistralChatEndpointAsync(chat, textExecutionSettings).ConfigureAwait(false);

            // ChatMessageContent content = new(AuthorRole.Assistant, responseData.choices[0].message.content);

            IReadOnlyDictionary<string, object?> metadata = new Dictionary<string, object?>()
        {
            {"Usage", responseData.usage }
        };
            var defaultResult = responseData.choices.Select(chatChoice => new ChatMessageContent(
         role: AuthorRole.Assistant,
         content: chatChoice.message.content,
          modelId: responseData.model,
          metadata: metadata
       )).ToList();

            var toolCalls = responseData.choices[0].message.tool_calls;

            if (!autoInvoke || toolCalls == null || toolCalls.Count == 0)
            {
                //nothing to call, return as normal result
                return defaultResult;
            }

            //Some tool calling going on

            MistralAssitantMessageContent assistantResult = new(AuthorRole.Assistant, responseData);
            //adding to the history
            chat.Add(assistantResult);

            // We must send back a response for every tool call, regardless of whether we successfully executed it or not.
            // If we successfully execute it, we'll add the result. If we don't, we'll add an error.

            for (int i = 0; i < toolCalls.Count; i++)
            {
                var toolCall = toolCalls[i];
                tool_call_function functionCall = toolCall.function;

                // Parse the function call arguments.
                MistralFunctionToolCall? openAIFunctionToolCall;

                try
                {
                    openAIFunctionToolCall = new(toolCall.function);
                }
                catch (JsonException)
                {
                    chat.Add(new MistralToolMessageContent(toolCall.function.name, "Error: Function call arguments were invalid JSON.")); //todo: use metadata and better logging
                    continue;
                }

                //todo: check if not allowed any function to be called, and if the proposed function is in the list of possible functions to be called

                // Find the function in the kernel and populate the arguments.
                if (!kernel!.Plugins.TryGetFunctionAndArguments(openAIFunctionToolCall, out KernelFunction? function, out KernelArguments? functionArgs))
                {
                    chat.Add(new MistralToolMessageContent(toolCall.function.name, $"Error: Requested function '{openAIFunctionToolCall.FullyQualifiedName}' could not be found.")); //todo: use metadata and better logging
                    continue;
                }

                // Now, invoke the function, and add the resulting tool call message to the chat options.
                s_inflightAutoInvokes.Value++;
                object? functionResult;
                try
                {
                    // Note that we explicitly do not use executionSettings here; those pertain to the all-up operation and not necessarily to any
                    // further calls made as part of this function invocation. In particular, we must not use function calling settings naively here,
                    // as the called function could in turn telling the model about itself as a possible candidate for invocation.
                    functionResult = (await function.InvokeAsync(kernel, functionArgs, cancellationToken: cancellationToken).ConfigureAwait(false)).GetValue<object>() ?? string.Empty;

                    //it worked, add the result to the ChatHistory so the LLM can work with it

                    chat.Add(new MistralToolMessageContent(toolCall.function.name, functionResult as string ?? JsonSerializer.Serialize(functionResult)));
                }
#pragma warning disable CA1031 // Do not catch general exception types
                catch (Exception e)
#pragma warning restore CA1031
                {
                    chat.Add(new MistralToolMessageContent(toolCall.function.name, $"Error: Exception while invoking function: {e.Message}")); //todo: use metadata and better logging                  
                    continue;
                }
                finally
                {
                    s_inflightAutoInvokes.Value--;
                }
            }

            // Respect the tool's maximum use attempts and maximum auto-invoke attempts.
            Debug.Assert(textExecutionSettings.ToolCallBehavior is not null);

            if (iteration >= textExecutionSettings.ToolCallBehavior!.MaximumAutoInvokeAttempts)
            {
                autoInvoke = false;
                if (this.Logger.IsEnabled(LogLevel.Debug))
                {
                    this.Logger.LogDebug("Maximum auto-invoke ({MaximumAutoInvoke}) reached.", textExecutionSettings.ToolCallBehavior!.MaximumAutoInvokeAttempts);
                }
            }
        }
    }

    private void AddToolCallsToPromptExecutionSettings(MistralPromptExecutionSettings executionSettings, Kernel? kernel)
    {
        executionSettings.ToolCallBehavior?.ConfigureOptions(kernel, executionSettings);
    }

    private async Task<MistralAIChatEndpointResponse> CallMistralChatEndpointAsync(ChatHistory chat, MistralPromptExecutionSettings textExecutionSettings)
    {
        List<Message> messages = PrepareChatMessages(chat);

        MistralAiChatEndpointRequest request = new()
        {
            Model = this.DeploymentOrModelName,
            Stream = false,
            Messages = messages.ToArray()
        };
        request.ApplySettings(textExecutionSettings);

        string requestJson = JsonSerializer.Serialize(request);
        using (var content = new StringContent(requestJson, Encoding.UTF8, "application/json"))
        {
            string url = "https://api.mistral.ai/v1/chat/completions";
            var response = await this.CallMistralAuthedAsync(url, content).ConfigureAwait(false);
            var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);

            var result = JsonSerializer.Deserialize<MistralAIChatEndpointResponse>(responseContent!);
            return result!;
        }
    }

    private static List<Message> PrepareChatMessages(ChatHistory chat)
    {
        //TODO: add tool calls here
        List<Message> messages = new();
        foreach (var msg in chat)
        {
            Message message = new("assistant", msg.Content);

            switch (msg.Role)
            {
                case var role when role.Equals(AuthorRole.User):
                    message.role = "user";
                    break;
                case var role when role.Equals(AuthorRole.System):
                    message.role = "system";
                    break;
                case var role when role.Equals(AuthorRole.Tool):
                    message.role = "tool";
                    if (msg is MistralToolMessageContent)
                    {
                        message.tool_name = ((MistralToolMessageContent)msg).FunctionName;
                    }
                    break;
                case var role when role.Equals(AuthorRole.Assistant):
                    message.role = "assistant";
                    if (msg is MistralAssitantMessageContent)
                    {
                        message.tool_calls = ((MistralAssitantMessageContent)msg).ToolCalls!.ToList();
                    }
                    break;
                default:
                    // Handle the default case here if necessary
                    break;
            }

            messages.Add(message);
        }
        //except for the first message there can not be any other system messages

        for (int i = 1; i < messages.Count; i++)
        {
            if (messages[i].role == "system")
            {
                messages[i].role = "user";
            }
        }
        //hack: the API does not like system-only requests, but the InvokePrompt does that by default
        if (messages.Last().role != "user" && messages.Last().role != "tool")
        {
            messages.Last().role = "user";
        }

        return messages;
    }

    private async Task<MistralAIEmbeddingEndpointResponse> CallMistralEmbeddingsEndpointAsync(string[] inputs)
    {
        MistralAIEmbeddingEndpointRequest embeddingRequest = new(this.DeploymentOrModelName, inputs);

        using (var content = new StringContent(JsonSerializer.Serialize(embeddingRequest), Encoding.UTF8, "application/json"))
        {
            string url = "https://api.mistral.ai/v1/embeddings";
            var response = await this.CallMistralAuthedAsync(url, content).ConfigureAwait(false);
            var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);
            if (string.IsNullOrEmpty(responseContent))
            {
                throw new InvalidOperationException("Invalid response received from Mistral API server");
            }

            var result = JsonSerializer.Deserialize<MistralAIEmbeddingEndpointResponse>(responseContent);
            return result!;
        }
    }
    private async Task<HttpResponseMessage> CallMistralStreamingEndpointAsync(ChatHistory chat, MistralPromptExecutionSettings textExecutionSettings)
    {
        List<Message> messages = PrepareChatMessages(chat);

        MistralAiChatEndpointRequest request = new()
        {
            Model = this.DeploymentOrModelName,
            Stream = true,
            Messages = messages.ToArray()
        };
        request.ApplySettings(textExecutionSettings);
        string requestJson = JsonSerializer.Serialize(request);
        using (var content = new StringContent(requestJson, Encoding.UTF8, "application/json"))
        {
            string url = "https://api.mistral.ai/v1/chat/completions";
            var response = await this.CallMistralAuthedAsync(url, content).ConfigureAwait(false);
            return response;
        }
    }

    private async Task<HttpResponseMessage> CallMistralAuthedAsync(string url, StringContent content)
    {
        this._httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", this._apiKey);

        var response = await this._httpClient.PostAsync(url, content).ConfigureAwait(false);
        if (!response.IsSuccessStatusCode)
        {
            var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);
            throw new HttpOperationException(response.StatusCode, responseContent, response.ReasonPhrase, new HttpRequestException());
        }

        return response;
    }
    internal async IAsyncEnumerable<StreamingTextContent> GetChatAsTextStreamingContentsAsync(
       string prompt,
       PromptExecutionSettings? executionSettings,
       Kernel? kernel,
    [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        ChatHistory history = new();
        history.AddUserMessage(prompt);

        await foreach (var chatUpdate in this.GetStreamingChatMessageContentsAsync(history, executionSettings, kernel, cancellationToken))
        {
            yield return new StreamingTextContent(chatUpdate.Content, chatUpdate.ChoiceIndex, chatUpdate.ModelId, chatUpdate, Encoding.UTF8, chatUpdate.Metadata);
        }
    }

    internal async IAsyncEnumerable<StreamingChatMessageContent> GetStreamingChatMessageContentsAsync(
        ChatHistory chat,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
       [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        MistralPromptExecutionSettings textExecutionSettings = MistralPromptExecutionSettings.FromExecutionSettings(executionSettings, MistralPromptExecutionSettings.DefaultTextMaxTokens);

        StringBuilder contentBuilder = new();
        // Make the request.
        HttpResponseMessage streamingResponse = await this.CallMistralStreamingEndpointAsync(chat, textExecutionSettings).ConfigureAwait(false);
        Stream responseStream = await streamingResponse.Content.ReadAsStreamAsync().ConfigureAwait(false);

        using var reader = new StreamReader(responseStream);
        bool finished = false;
        while (!reader.EndOfStream && !finished)
        {
            var line = await reader.ReadLineAsync().ConfigureAwait(false);
            if (string.IsNullOrEmpty(line))
            {
                continue;
            }
            if (line.StartsWith("data: ", StringComparison.InvariantCulture))
            {
                line = line.Substring(6);
            }
            var response = JsonSerializer.Deserialize<MistralAIChatStreamingResponse>(line);

            foreach (var choice in response!.choices)
            {
                contentBuilder.Append(choice.delta.content);
                var metadata = GetResponseMetadata(choice);
                yield return new MistralStreamingChatMessageContent(choice.index, this.DeploymentOrModelName, choice.delta.content, metadata);
                finished = !string.IsNullOrEmpty(choice.finishReason);
            }
        }

        // Get any response content that was streamed.
        string content = contentBuilder?.ToString() ?? string.Empty;

        chat.Add(new ChatMessageContent(AuthorRole.Assistant, content, modelId: this.DeploymentOrModelName));
    }

    private static Dictionary<string, object?> GetResponseMetadata(MistralAPI.Choice choice)
    {
        return new Dictionary<string, object?>(2)
        {
            { nameof(choice.index), choice.index },
            { nameof(choice.delta.role), choice.delta.role }
        };
    }

    internal async Task<IReadOnlyList<TextContent>> GetChatAsTextContentsAsync(
        string text,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {
        ChatHistory chat = new();
        chat.AddUserMessage(text);
        return (await this.GetChatMessageContentsAsync(chat, executionSettings, kernel, cancellationToken).ConfigureAwait(false))
            .Select(chat => new TextContent(chat.Content, chat.ModelId, chat.Content, Encoding.UTF8, chat.Metadata))
            .ToList();
    }

    internal void AddAttribute(string key, string? value)
    {
        if (!string.IsNullOrEmpty(value))
        {
            this.Attributes.Add(key, value);
        }
    }

    internal void LogActionDetails([CallerMemberName] string? callerMemberName = default)
    {
        if (this.Logger != null && this.Logger.IsEnabled(LogLevel.Information))
        {
            this.Logger.LogInformation("Action: {Action}. Mistral Model ID: {ModelId}.", callerMemberName, this.DeploymentOrModelName);
        }
    }

    internal async Task<IList<ReadOnlyMemory<float>>> GetEmbeddingsAsync(IList<string> dataIn, Kernel? kernel, CancellationToken cancellationToken)
    {
        MistralAIEmbeddingEndpointResponse embeddingResponse = await this.CallMistralEmbeddingsEndpointAsync(dataIn.ToArray()).ConfigureAwait(false);
        var result = new List<ReadOnlyMemory<float>>(dataIn.Count);

        foreach (var data in embeddingResponse.Data.ToArray())
        {
            result.Add(data.Embedding);
        }

        return result;
    }
}
