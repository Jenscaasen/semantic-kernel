// Copyright (c) Microsoft. All rights reserved.

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Diagnostics.Metrics;
using System.Linq;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Runtime.CompilerServices;
using System.Text;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.Mistral.AzureSdk;
using Microsoft.SemanticKernel.Connectors.Mistral.MistralAPI;
using Microsoft.SemanticKernel.Http;

#pragma warning disable CA2208 // Instantiate argument exceptions correctly

namespace Microsoft.SemanticKernel.Connectors.Mistral;

/// <summary>
/// Base class for AI clients that provides common functionality for interacting with OpenAI services.
/// </summary>
internal class MistralClientCore
{
    private const int MaxResultsPerPrompt = 128;

    internal MistralClientCore(string modelName, string apiKey, HttpClient httpCient = null)
    {
        _apiKey = apiKey;
        this._httpCient = httpCient;
        DeploymentOrModelName = modelName;
        
    }

    /// <summary>
    /// Model Id or Deployment Name
    /// </summary>
    internal string DeploymentOrModelName { get; set; } = string.Empty;

    /// <summary>
    /// Logger instance
    /// </summary>
    internal ILogger Logger { get; set; }

    private string _apiKey;
    private HttpClient _httpCient;

    /// <summary>
    /// Storage for AI service attributes.
    /// </summary>
    internal Dictionary<string, object?> Attributes { get; } = new();

    /// <summary>
    /// Creates completions for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="executionSettings">Execution settings for the completion API.</param>
    /// <param name="kernel">The <see cref="Kernel"/> containing services, plugins, and other state for use throughout the operation.</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>Completions generated by the remote model</returns>
    internal async Task<IReadOnlyList<TextContent>> GetTextResultsAsync(
        string text,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {
        ChatHistory history = new ChatHistory();
        history.AddUserMessage(text);
        var chatResult = await GetChatMessageContentsAsync(history, executionSettings, kernel, cancellationToken).ConfigureAwait(false);
        return chatResult.Select(choice => new TextContent(choice.Content, this.DeploymentOrModelName, choice, Encoding.UTF8, choice.Metadata)).ToList();

    }

    internal  IAsyncEnumerable<StreamingTextContent> GetStreamingTextContentsAsync(
        string prompt,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        throw new Exception("Not supported by API");
    }

    /// <summary>
    /// Generate a new chat message
    /// </summary>
    /// <param name="chat">Chat history</param>
    /// <param name="executionSettings">Execution settings for the completion API.</param>
    /// <param name="kernel">The <see cref="Kernel"/> containing services, plugins, and other state for use throughout the operation.</param>
    /// <param name="cancellationToken">Async cancellation token</param>
    /// <returns>Generated chat message in string format</returns>
    internal async Task<IReadOnlyList<ChatMessageContent>> GetChatMessageContentsAsync(
        ChatHistory chat,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {
       
        // Make the request.
        var responseData = await CallMistralChatEndpointAsync(chat).ConfigureAwait(false);

        ChatMessageContent content = new ChatMessageContent(AuthorRole.Assistant, responseData.choices[0].message.content);

        IReadOnlyDictionary<string, object> metadata = new Dictionary<string, object>()
        {
            {"Usage", responseData.usage }
        };
        return responseData.choices.Select(chatChoice => new ChatMessageContent(
           role: AuthorRole.Assistant,
           content: chatChoice.message.content,
            modelId: responseData.model,
            metadata: metadata
         )).ToList();
    }

    private async Task<MistralAIChatEndpointResponse> CallMistralChatEndpointAsync(ChatHistory chat)
    {
      
        List<Message> messages = new List<Message>();
        foreach(var msg in chat)
        {
            string role = "assistant";
            if (msg.Role == AuthorRole.User) role = "user";
            if (msg.Role == AuthorRole.System) role = "system";
            messages.Add(new Message { role = role, content = msg.Content });
        }

        //hack: the API does not like system-only requests
        if(messages.Last().role != "user") messages.Last().role = "user";

        MistralAiChatEndpointRequest request = new MistralAiChatEndpointRequest {
            model = DeploymentOrModelName,
            safe_mode = true,
            messages = messages.ToArray()
        };
        var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, "application/json");

        string url = "https://api.mistral.ai/v1/chat/completions";       
        var responseContent = await CallMistralAuthedAsync(url, content).ConfigureAwait(false);
        
        MistralAIChatEndpointResponse result = JsonSerializer.Deserialize< MistralAIChatEndpointResponse>(responseContent);
        return result;
    }
    private async Task<MistralAIEmbeddingEndpointResponse> CallMistralEmbeddingsEndpointAsync(string[] inputs)
    {
        MistralAIEmbeddingEndpointRequest embeddingRequest = new() { model = DeploymentOrModelName,  input = inputs }; 

        var content = new StringContent(JsonSerializer.Serialize(embeddingRequest), Encoding.UTF8, "application/json");

        string url = "https://api.mistral.ai/v1/embeddings";
        var responseContent = await this.CallMistralAuthedAsync(url, content).ConfigureAwait(false);
    
        MistralAIEmbeddingEndpointResponse result = JsonSerializer.Deserialize<MistralAIEmbeddingEndpointResponse>(responseContent);
        return result;
    }
    private async Task<string> CallMistralAuthedAsync(string url, StringContent content)
    {
        if (_httpCient == null)
        {
            _httpCient = new HttpClient();
        }
        _httpCient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", _apiKey);

        var response = await _httpCient.PostAsync(url, content).ConfigureAwait(false);
        var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);
        if (!response.IsSuccessStatusCode)
        {
            throw new HttpOperationException(response.StatusCode, responseContent, response.ReasonPhrase, new HttpRequestException());
        }

        return responseContent;
    }
    internal  IAsyncEnumerable<StreamingChatMessageContent> GetStreamingChatMessageContentsAsync(
        ChatHistory chat,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        throw new Exception("Not supported by API");
    }

    internal  IAsyncEnumerable<StreamingTextContent> GetChatAsTextStreamingContentsAsync(
        string prompt,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        throw new Exception("Not supported by API");
    }

    internal async Task<IReadOnlyList<TextContent>> GetChatAsTextContentsAsync(
        string text,
        PromptExecutionSettings? executionSettings,
        Kernel? kernel,
        CancellationToken cancellationToken = default)
    {

        ChatHistory chat = new ChatHistory();
        chat.AddUserMessage(text);
        return (await this.GetChatMessageContentsAsync(chat, executionSettings, kernel, cancellationToken).ConfigureAwait(false))
            .Select(chat => new TextContent(chat.Content, chat.ModelId, chat.Content, Encoding.UTF8, chat.Metadata))
            .ToList();
    }

    internal void AddAttribute(string key, string? value)
    {
        if (!string.IsNullOrEmpty(value))
        {
            this.Attributes.Add(key, value);
        }
    }

    internal void LogActionDetails([CallerMemberName] string? callerMemberName = default)
    {
        if (this.Logger !=null && this.Logger.IsEnabled(LogLevel.Information))
        {
            this.Logger.LogInformation("Action: {Action}. Mistral Model ID: {ModelId}.", callerMemberName, this.DeploymentOrModelName);
        }
    }

    internal async Task<IList<ReadOnlyMemory<float>>> GetEmbeddingsAsync(IList<string> dataIn, Kernel? kernel, CancellationToken cancellationToken)
    {
        MistralAIEmbeddingEndpointResponse embeddingResponse = await this.CallMistralEmbeddingsEndpointAsync(dataIn.ToArray()).ConfigureAwait(false);
        var result = new List<ReadOnlyMemory<float>>(dataIn.Count);
       
        foreach (var data in embeddingResponse.data)
        {
            result.Add(data.embedding);
        }

        return result;
    }
}

